---
title: "Simulating the 2020 Election With Markets"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.width = 9,
  fig.height = 5,
  dpi = 300
)
options(width = 99)
set.seed(seed = 05)
```

Election prediction helps party officials, campaign operatives, and journalists interpret campaigns
in a quantitative manner. In the past few years, the forecasting model has become a staple of
political punditry as big data has sought to supplant arbitrary punditry. Popularized by the data
journalist at FiveThirtyEight, the forecasting model is a statistical tool used to incorporate a
number of quantitative inputs and produce a _probabilistic_ view of all possible outcomes.

However, following the 2016 Presidential election, the public ([wrongly][01]) felt betrayed by promise of data to predict the future. This left political scientists and journalists alike
reassessing other predictive tools. My favorite of these alternatives is the prediction market.

Prediction markets can be used to generate similarly probabilistic views of election outcomes by
utilizing the economic forces of price discovery and risk aversion to overcome the ideological
bias of self-interested traders on a binary options exchange. Traders use real money to buy shares
of [futures contracts][02] tied to an outcome. The price of these shares fluctuates on the market
as the underlying _likelihood_ of that outcome changes. [PredictIt][03] is an exchange for such contracts, run by Victoria University of Wellington.

Following the 2018 Midterm elections, [I wrote a paper][04] comparing these markets to the
congressional model published by the data journalists at FiveThirtyEight. I found no statistical
difference in the two method's ability to make [skilled predictions][05] over the course of the
Midterm elections. In fact, the markets showed reasonable skepticism in a number of upset
elections. Below, you can see how competitive Congressonal races were predicted by both the markets and model.

![](https://raw.githubusercontent.com/kiernann/models-markets/master/plots/plot_cartesian.png)

With the 2020 Presidential race well under way, the media, voters, campaigns, and political
scientists alike are all looking for the best way to provide useful predictions and avoid the
pitfalls of 2016. This far from the General Election, what little polling we have 
[is less than useless][06]. In the absense of more quantitative data, can we possibly use
prediction markets to generate a useful probabilistic simulation of the electoral college? today 
I'll try and use data from the PredictIt exchange to answer this question.

[01]: http://53eig.ht/2fIYJK2
[02]: https://en.wikipedia.org/wiki/Futures_contract
[03]: https://www.predictit.org/
[04]: https://github.com/kiernann/models-markets
[05]: https://en.wikipedia.org/wiki/Brier_score
[06]: https://53eig.ht/2IFHxVW

## Code

I'll be using the open source [language R][07] and packages from the [Tidyverse ecosystem][08].

[07]: https://www.r-project.org/
[08]: https://www.tidyverse.org/

```{r packages, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(
  tidyverse,
  magrittr,
  jsonlite,
  janitor,
  scales,
  rvest,
  usmap
)
```

## Market Data

PredictIt hosts markets for most of the competitive battleground states. We can scrape these
markets using their API and the `jsonlite::fromJSON()` function.

```{r scrape_markets}
market_prices <-
  fromJSON(txt = "https://www.predictit.org/api/marketdata/all/") %>%
  use_series(markets) %>%
  filter(str_detect(name, "Which party will win (.*) in the 2020 presidential election?")) %>%
  unnest(contracts, names_repair = make_clean_names) %>%
  filter(short_name_2 == "Democratic") %>%
  select(state = short_name, price = last_close_price) %>%
  mutate(state = str_extract(state, "[:upper:]{2}")) %>% 
  arrange(price)
```

From this API, we get probability data for `r nrow(market_prices)` battleground states.

```{r map_markets, echo=FALSE}
brewer_rdbu <- RColorBrewer::brewer.pal(n = 5, name = "RdBu")
usa_map <- map_data("state") %>% mutate(state = abrev_state(region))
states_ec_map <- left_join(x = usa_map, y = market_prices)
states_ec_map %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = price)) +
  coord_quickmap() +
  scale_fill_gradient2(
    low = brewer_rdbu[1],
    high = brewer_rdbu[5],
    mid = brewer_rdbu[3],
    midpoint = 0.50,
    labels = scales::dollar
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Battleground Market Prices",
    subtitle = "Closing price of democratic candidate futures contract",
    caption = "Source: PredictIt"
  )
```

These states alone aren't enough to simulate the electoral college in 2020. To predict the
remaining states, we have a few options. The easiest route is to simply assume the party which
won in 2016 will win again in 2020. This isn't a terrible idea, as the majority of states rarely
flip, especially not the `r 50-nrow(market_prices)` states without a prediction market. We can start
from this assumption and improve upon it very easily.

## Past Data

We can scrape the 2016 election results from Wikipedia, where we can find the percentage of the 
popular vote as well as the number of electoral college votes up for grab in each state.

```{r scrape_past}
past_results <-
  # scrape table
  read_html("https://en.wikipedia.org/wiki/2016_United_States_presidential_election") %>%
  html_node("table.wikitable:nth-child(1)") %>%
  html_table(fill = TRUE) %>%
  na_if("â€“") %>%
  as_tibble(.name_repair = "unique") %>%
  # select columns and rows
  select(1, 3, 5, 6, 8) %>%
  slice(-1, -58, -59) %>%
  set_names(c("state", "dem", "dem_votes", "rep", "rep_votes")) %>%
  # parse numeric cols
  map_dfc(parse_guess) %>%
  mutate(
    # coalesce EC votes won by both
    votes = coalesce(dem_votes, rep_votes),
    # calculate the dem share
    dem_prop = dem/(dem + rep),
    # abbreviate the state names
    state = state %>% 
      str_remove("\\(at-lg\\)") %>% 
      str_remove(",\\s\\d..$") %>% 
      abrev_state()
  ) %>% 
  # use mean for ME and NE
  group_by(state) %>% 
  summarize(
    past = mean(dem_prop),
    votes = sum(votes)
  )
```

```{r 2016_map, echo=FALSE}
left_join(usa_map, past_results) %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = past)) +
  coord_quickmap() +
  scale_fill_gradient2(
    low = brewer_rdbu[1],
    high = brewer_rdbu[5],
    mid = brewer_rdbu[3],
    midpoint = 0.50,
    labels = scales::percent
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "2016 Popular Vote",
    subtitle = "Democratic share of major party votes",
    caption = "Source: PredictIt"
  )
```

## Probabilities

Any good election forecast needs to be _probabilistic_. Professional forecasts take this division
of votes (usually from an aggregate of polls) then calculate the probability distribution around
that range with a series of other factors.

For this simulation, we already have probabilities for `r nrow(market_prices)` states. PredictIt
only hosts markets for the most competitive states. The reality is, the other 
`r 51-nrow(market_prices)` contests are fairly noncompetitive. From the density plot below, we can
see how the 2016 popular vote differed for those states _with_ 2020 markets and those without.

```{r vote_range, echo=FALSE}
past_results %>%
  filter(state != "DC") %>%
  mutate(
    market = if_else(
      condition = state %in% market_prices$state,
      true = "No Market",
      false = "Market Exists"
    )
  ) %>% 
  ggplot(aes(x = past)) + 
  geom_density(aes(fill = market)) +
  facet_wrap(~market, ncol = 1, scales = "free_y") +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    guide = FALSE,
    values = c(
      RColorBrewer::brewer.pal(3, "Dark2")[1],
      RColorBrewer::brewer.pal(3, "Dark2")[2]
    )
  ) +
  labs(
    title = "Popular Vote Difference for Battleground States",
    x = "2016 Popular Vote",
    y = "Number of States"
  ) +
  theme(strip.text = element_text(size = 12))
```

For those states without a market, we need to convert these vote shares to probabilities. In
reality, a even a 5% edge in the popular vote results in a significant advantage and a probability
difference much greater than 5%.

To make this conversion, we simulate many new election using the 2016 result. We make these
simulations using a normal distribution around the 2016 result. The standard deviation of such a 
distribution is where uncertainty is introduced, converting our popular vote to a probability.

More complicated forecasting models like those released by FiveThirtyEight use proprietary models
to calculate this degree of uncertainty. We know these models incorperate a number of historically 
predictive variables (partisanship, fundraising, incumbency, etc). For this simple demonstration,
I will calculate the standard deviation of the past 11 Presidential elections in each state.

The MIT Dataverse contains a dataset of Presidential contests since 1976 with party vote counts in
each state. We can read the file using `readr::read_tsv()` and calulating the standard deviation
with `dplyr::group_by()` and `dplyr::summarize()`.

```{r read_mit}
mit_url <- "https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/42MVDX/MFU99O"
# MIT Election Data and Science Lab, 2017, "1976-2016-president.tab"
# https://doi.org/10.7910/DVN/42MVDX/MFU99O, Harvard Dataverse, V5

potus <- read_delim(
    file = mit_url,
    delim = "\t",
    escape_double = FALSE,
    escape_backslash = TRUE
  )
```

```{r summarize_sd}
states_sd <- potus %>%
  filter(candidatevotes > 1000) %>% 
  filter(party %in% c("democrat")) %>%
  mutate(prop = candidatevotes/totalvotes,) %>%
  select(year, state = state_po, prop) %>%
  group_by(state) %>%
  summarize(sd = sd(prop))
```

Standard deviation is the variation in election results. A state with little change year to year
will have a lower standard deviation, increasing our confidence and decreasing uncertainty in the
next election/.

```{r}
left_join(usa_map, states_sd) %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = sd)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "seq", palette = 5, direction = 1
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Variation in Democratic Share",
    subtitle = "Presidential Elections 1976-2016",
    fill = "Standard\nDeviation",
    caption = "Source: MIT Election Data and Science Lab"
    )
```

Using this standard deviation, we can generate 10,000 values from this random normal distribution.
We can think of each of these values as a simulated election. The percentage of simulated elections
won is the same as the _probability_ of winning.

```{r add_sd_rnorm}
past_results <- past_results %>% 
  left_join(states_sd) %>%
  rowwise() %>% 
  mutate(prob = mean(rnorm(10000, past, sd) > 0.50))
```

If we visualize this process, we can see how the 2016 result and a standard deviation is used
to simulate many elections and calculate a probability.

Below you can see the results of 1,000 simulated elections in Maryland, where
`r scales::percent(past_results$past[past_results$state == "MD"])` of votes were cast for the
Democratic candidate in the last election. The area under the curve past 50% is the _probability_
of a democrat winning again in the next election.

```{r example_range_md, echo=FALSE}
ex_past <- past_results$past[past_results$state == "MD"]
ex_sd <- past_results$sd[past_results$state == "MD"]
n <- 1000
ggplot(data = NULL, mapping = aes(x = rnorm(n = n, mean = ex_past, sd = ex_sd))) + 
  geom_density(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  geom_vline(xintercept = 0.5, size = 1) +
  geom_vline(xintercept = ex_past, linetype = 2, size = 1) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = sprintf("Maryland 2020 Simulated %s Times", comma(n)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  ) +
  geom_label(aes(x = 0.5, y = 4, label = "Winning Threshold")) +
  geom_label(aes(x = ex_past, y = 5, label = "2016 Result")) +
  theme(
    plot.title = element_text(hjust = 0.50),
    axis.text.y.left = element_blank(),
    axis.ticks.y.left = element_blank()
  )
```

Now, lets see the distribution of 1,000 simulated elections in Florida, a much closer election with
only `r scales::percent(past_results$past[past_results$state == "FL"])` of voters supporting the
democratic candidate.

```{r example_range_fl, echo=FALSE}
ex_past <- past_results$past[past_results$state == "FL"]
ex_sd <- past_results$sd[past_results$state == "FL"]
n <- 1000
ggplot(data = NULL, mapping = aes(x = rnorm(n = n, mean = ex_past, sd = ex_sd))) + 
  geom_density(fill = RColorBrewer::brewer.pal(3, "Dark2")[2]) +
  geom_vline(xintercept = 0.5, size = 1) +
  geom_vline(xintercept = ex_past, linetype = 2, size = 1) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = sprintf("Folrida 2020 Simulated %s Times", comma(n)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  ) +
  geom_label(aes(x = 0.5, y = 4, label = "Winning Threshold")) +
  geom_label(aes(x = ex_past, y = 5, label = "2016 Result")) +
  theme(
    plot.title = element_text(hjust = 0.50),
    axis.text.y.left = element_blank(),
    axis.ticks.y.left = element_blank()
  )
```

We can generate this probability by calculating the average number of simulated elections won by
the democrat. Below, we see how this is done by simulating the Connecticut election 30 times.

```{r example_process}
(ex_past <- past_results$past[past_results$state == "CT"])
(ex_sd <- past_results$sd[past_results$state == "CT"])
(ex_sims <- round(x = rnorm(n = 30, mean = ex_past, sd = ex_sd), digits = 2))
(ex_wins <- ex_sims > 0.5)
mean(ex_wins)
```

Below, you can see how the 2016 vote results result in more extreme probabilities.

```{r past_results_hist, echo=FALSE}
past_results %>% 
  mutate(dem = past > 0.5) %>% 
  ggplot(aes(x = past)) +
  geom_histogram(bins = 10, mapping = aes(fill = dem)) +
  geom_vline(xintercept = 0.5, size = 1) +
  labs(
    title = "Popular Vote Distribution",
    x = "Popular Vote",
    y = "2016 States"
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = RColorBrewer::brewer.pal(8, "RdBu")[c(1, 8)],
    guide = FALSE
  ) +
  theme(plot.title = element_text(hjust = 0.50))
```

```{r sim_prob_hist, echo=FALSE}
past_results %>% 
  mutate(dem = prob > 0.5) %>% 
  ggplot(aes(x = prob)) +
  geom_histogram(bins = 10, mapping = aes(fill = dem)) +
  geom_vline(xintercept = 0.5) +
  labs(
    title = "Simulated Probability Distribution",
    x = "Popular Vote",
    y = "2016 States"
  ) +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = RColorBrewer::brewer.pal(8, "RdBu")[c(1, 8)],
    guide = FALSE
  ) +
  theme(plot.title = element_text(hjust = 0.50))
```

This relationship depends entirely on our chosen standard deviation. Again, since we are only
simulating probabilities for those states _without_ markets, these probabilities tend to be extreme.

```{r sim_relationship, echo=FALSE}
past_results %>% 
  filter(state %out% market_prices$state) %>% 
  ggplot(aes(x = past, y = prob)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, size = votes, fill = prob > 0.5)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(8, "RdBu")[c(1, 8)], guide = FALSE) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  )
```

## Combine Sources

The efficient market hypothesis holds that our markets are a more accurate method to generate
probabilistic predictions. We will uses these market prices over our simulated elections where we
have them.

```{r join_past_price}
ec <- past_results %>% 
  left_join(market_prices, by = "state") %>% 
  mutate(
    dem = coalesce(price, prob),
    market = !is.na(price)
  ) %>% 
  select(state, dem, market, votes)
```

```{r sim_relationship_market, echo=FALSE}
past_results %>% 
  left_join(market_prices, by = "state") %>% 
  mutate(
    dem = coalesce(price, prob),
    market = !is.na(price)
  ) %>% 
  ggplot(aes(x = past, y = dem)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, size = votes, fill = market)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(3, "Dark2")[c(1, 2)]) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    fill = "Market Price",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  ) +
  theme(legend.position = "bottom")
```

```{r 2020_map, echo=FALSE}
left_join(usa_map, ec) %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = dem)) +
  coord_quickmap() +
  scale_fill_gradient2(
    low = brewer_rdbu[1],
    high = brewer_rdbu[5],
    mid = "white",
    midpoint = 0.50,
    labels = scales::percent
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Probability of Democratic Victory",
    subtitle = "Simulated and Market Prices",
    fill = "Probability",
    caption = "Source: Wikipedia, MIT, & PredictIt"
    )
```

## Electoral College

To simulate the entire electoral college, we simple have to perform the same `sample()` process as
we did with Connecticut, above. To simplify this process, we can create a new `sim_race()` function
that takes a probability and returns a `TRUE` or `FALSE` indicating whether or not the democrat
has won.

```{r sim_fun}
sim_race <- function(dem = 1-rep, rep = 1-dem) {
  sample(
    size = 1,
    x = c(TRUE, FALSE),
    prob = c(dem, rep)
  )
}
```

With this function, we can simulate every state in the country and count the number of electoral
college votes won by each party.

```{r sim1}
sim1 <- map_lgl(ec$dem, sim_race)
sum(ec$votes[sim1])
```

```{r}
sim1_result <- if_else(
  condition = sum(ec$votes[sim1]) > 269,
  true = "the Democrats did win",
  false = "the Democrats did not win"
)
```

In the above election, `r sim1_result`. 

To best understand the _range_ of possible outcomes, we can perform the same simulation many times.

```{r}
n <- 100000
sims <- rep(NA, n)
for (i in seq(1, n)) {
  state_outcomes <- map_lgl(ec$dem, sim_race)
  dem_total <- sum(ec$votes[state_outcomes])
  sims[i] <- dem_total
}
```

From the summary below, you can see a picture of a very close race with the Democrats holding a 
slight lead. Of our `r scales::comma(n)` simulations, the Democrats won 
`r scales::percent(mean(sims > 269))` with the modal outcome being a victory of 
`r names(sort(table(sims), decreasing = TRUE)[1])` electoral college votes.

```{r summary_sims}
summary(sims)
mean(sims > 269)
```

```{r sim_hist, echo=FALSE}
ggplot(data = NULL) +
  geom_histogram(mapping = aes(x = sims, fill = sims > 270)) +
  geom_vline(xintercept = 270, size = 1) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(8, "RdBu")[c(1, 8)], guide = FALSE) +
  labs(
    title = sprintf("%s Simulated Electoral College Outcomes", scales::comma(n)),
    x = "Simulated Democratic Votes",
    y = "Number of Simulations"
  ) 
```

