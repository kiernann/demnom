---
title: "Simulating the 2020 Election With Markets"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.width = 9,
  fig.height = 5,
  dpi = 300
)
options(width = 99)
set.seed(seed = 05)
```

Election prediction helps party officials, campaign operatives, and journalists interpret campaigns
in a quantitative manner. Understanding trends, uncertainty, and likely outcomes is in invaluable
political tool. For this reason, elections will always be predicted. In the absense of numbers, 
people will latch on to whatever predictive tool they can find. The [stock market][01], 
[key incumbency factors][02], the [Washington Redskins][03], [Halloween mask sales][04], and 
a [psychic Chinese monkeys][05]. If we're going to predict elections, we have a responsibility to
make legitimate quatitative predictions from scientifically founded bases. 

In the past few years, as big data has sought to supplant arbitrary punditry, the forecasting model
has become a staple of political science and journalism. Popularized by the data journalist at
[FiveThirtyEight][06], the forecasting model is a statistical tool used to incorporate a number of
quantitative inputs and produce a _probabilistic_ view of all possible outcomes.

However, following the 2016 Presidential election, the public ([perhaps wrongly][07]) felt betrayed
by the promise of data to predict the future. This left political scientists and journalists alike
reassessing other predictive tools. My favorite of these alternatives is the prediction market.

Prediction markets can be used to generate similarly probabilistic views of election outcomes by
utilizing the economic forces of price discovery and risk aversion to overcome the ideological
bias of self-interested traders on a binary options exchange. Traders use real money to buy shares
of [futures contracts][08] tied to an outcome. The price of these shares fluctuates on the market
as the underlying _likelihood_ of that outcome changes. [PredictIt][09] is an exchange for such contracts, run by Victoria University of Wellington.

Following the 2018 Midterm elections, [I wrote a paper][10] comparing these markets to the
congressional model published by the data journalists at FiveThirtyEight. I found no statistical
difference in the two method's ability to make [skilled predictions][11] over the course of the
Midterm elections. In fact, the markets showed reasonable skepticism in a number of upset
elections. Below, you can see how competitive Congressonal races were predicted by both the markets and model.

![](https://raw.githubusercontent.com/kiernann/models-markets/master/plots/plot_cartesian.png)

With the 2020 Presidential race well under way, the media, voters, campaigns, and political
scientists alike are all looking for the best way to provide useful predictions and avoid the
pitfalls of 2016. This far from the General Election, what little polling we have 
[is less than useless][12]. In the absense of more quantitative data, can we possibly use
prediction markets to generate a useful probabilistic simulation of the electoral college? today 
I'll try and use data from the PredictIt exchange to answer this question.

## Process

I'll be using the open source [language R][07] and packages from the [Tidyverse ecosystem][08].

```{r packages, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(tidyverse, dataverse, magrittr, jsonlite, scales, rvest)
```

Throughout this simulation, I'll be using only the Democratic party market prices, popular vote,
and probability. It's difficult to determine the winner of an election from a single party's 
share of the popular vote, as only a plurality and not a majority of votes is required. This means
we can't directly test if a party recieves >50% of the vote to determine if they've won, as some
winners recieve less than 50% of the vote. We can get around this by looking at only votes cast for
the major parties (Democrats and Republicans); if a party wins >50% of the major party vote, we can 
pretty safely assume they've won the plurality of votes. This is a reductive, but makes this quick
and dirty simulation a lot easier. A more professional model would fully incorperate third-party
voters, which [can affect elections][09].

## Battleground Data

PredictIt hosts markets for most of the competitive battleground states. We can scrape these
markets using their API and the `jsonlite::fromJSON()` function. 

```{r scrape_markets, warning=FALSE, error=FALSE}
market_prices <-
  # query predictit api
  fromJSON(txt = "https://www.predictit.org/api/marketdata/all/") %>%
  # use markets tree
  use_series(markets) %>%
  # keep only battleground markets
  filter(str_detect(name, "Which party will win (.*) in the 2020 presidential election?")) %>%
  # expose contracts nests has new rows
  unnest(contracts, names_repair = "unique") %>%
  # keep only dem contracts
  filter(shortName...11 == "Democratic") %>%
  # select state and latest price
  select(state = shortName...3, price = lastTradePrice) %>%
  # extract state abbreviation from question
  mutate(state = str_extract(state, "[:upper:]{2}")) %>% 
  arrange(price)
```

From this API, we get probability data for `r nrow(market_prices)` battleground states.

```{r map_markets, echo=FALSE}
brewer_rdbu <- RColorBrewer::brewer.pal(n = 5, name = "RdBu")
usa_map <- map_data("state") %>% mutate(state = abrev_state(region))
left_join(usa_map, market_prices) %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = price)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "div",
    palette = "RdBu",
    direction = 1,
    label = dollar
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank(),
    plot.caption = element_text(size = 11)
  ) +
  labs(
    title = "Battleground Market Prices",
    subtitle = sprintf("Market price of democratic contract on %s", date()),
    caption = "Source: PredictIt",
    fill = "Market\nPrice"
  )
```

We should note the trading volume of each market. Higher volume is typically associated with a more
accurate price equilibrium. That is, the more people trade on the market the more accurate the 
prediction will be. We can get this data by downloading the 90 day history of each market.

```{r market_volume, echo=FALSE}
college_markets <-
  fromJSON(txt = "https://www.predictit.org/api/marketdata/all/") %>%
  use_series(markets) %>%
  filter(str_detect(name, "Which party will win (.*) in the 2020 presidential election?"))

college_states <- str_extract(college_markets$shortName, "[:upper:]{2}")

college_urls <- glue("https://www.predictit.org/Resource/DownloadMarketChartData?marketid={college_markets$id}&timespan=90d")

map(
  college_urls,
  read_csv,
  col_types = cols(
    ContractName = col_character(),
    Date = col_date("%m/%d/%Y %H:%M:%S %p"),
    OpenSharePrice = col_number(),
    HighSharePrice = col_number(),
    LowSharePrice = col_number(),
    CloseSharePrice = col_number(),
    TradeVolume = col_double()
  )
) %>%
  set_names(value = college_states) %>%
  bind_rows(.id = "state") %>%
  rename(
    name   = ContractName,
    date   = Date,
    open   = OpenSharePrice,
    high   = HighSharePrice,
    low    = LowSharePrice,
    close  = CloseSharePrice,
    volume = TradeVolume
  ) %>% 
  group_by(state) %>% 
  arrange(date) %>% 
  mutate(vol_cum = cumsum(volume)) %>% 
  filter(
    date == max(college_data$date),
    name == "Democratic"
  ) %>% 
  right_join(usa_map) %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = vol_cum)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "seq",
    palette = "YlOrBr",
    direction = 1,
    label = comma
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank(),
    plot.caption = element_text(size = 11)
  ) +
  labs(
    title = "Cumulative Democratic Contracts Traded",
    subtitle = "Increased market volume associated with more accurate price equilibrium",
    caption = "Source: PredictIt",
    fill = "Cumulative\nVolume"
  )
```

## Safe States

These states alone aren't enough to simulate the 2020 electoral college. To predict the
remaining states, we have a few options. The easiest route is to simply assume the party which won
in 2016 will win again in 2020. This isn't a terrible idea, as the majority of states rarely flip,
especially not the `r 50-nrow(market_prices)` states without a prediction market. We can start from
this assumption and improve upon it very easily.

### Past Elections

To calculate some simple probablistic predictions for the remaining states, we will use data from
the MIT Election Data and Science Lab, which has a database of popular vote results in each state
for every Presidential election since 1976. This file can be read using `dataverse::get_file()`.

```{r read_mit}
past_elections <-
  # get MIT dataverse file
  get_file(
    file = "1976-2016-president.tab", 
    dataset = "doi:10.7910/DVN/42MVDX",
  ) %>%
  # parse raw as data frame
  read_csv(col_types = cols()) %>% 
  rename(votes = candidatevotes) %>%
  # keep only major candidates
  filter(writein == FALSE) %>% 
  filter(party %in% c("democrat", "republican")) %>% 
  # calculate dem share of major vote
  group_by(year, state_po) %>% 
  mutate(share = votes/sum(votes)) %>%
  # keep only dems
  filter(party == "democrat") %>% 
  select(year, state = state_po, share)

# MIT Election Data and Science Lab, 2017, "1976-2016-president.tab"
# https://doi.org/10.7910/DVN/42MVDX/MFU99O, Harvard Dataverse, V5
```

This historical data provides two statistics needed to finish our probablistic simulation. First,
we have the democratic share of the vote in the last election. Second, we can calculate the
variation in the party's share of the vote in the last `r n_distinct(past_elections$year)`
elections.

```{r summarize_sd}
state_sd <- past_elections %>% 
  # calculate std dev
  group_by(state) %>% 
  summarize(sd = sd(share))
```

```{r map_sd}
left_join(usa_map, state_sd, by = "state") %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = sd)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "seq", palette = 5, direction = 1
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Variation in Democratic Share of Major Party Vote",
    subtitle = "Presidential Elections, 1976-2016",
    fill = "Standard\nDeviation",
    caption = "Source: MIT Election Data and Science Lab"
    )
```

```{r last_election}
last_election <- past_elections %>%
  # keep only last election
  filter(year == 2016)
```

```{r 2016_map, echo=FALSE}
left_join(usa_map, last_election, by = "state") %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = share)) +
  coord_quickmap() +
  scale_fill_gradient2(
    low = brewer_rdbu[1],
    high = brewer_rdbu[5],
    mid = brewer_rdbu[3],
    midpoint = 0.50,
    labels = scales::percent
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    plot.caption = element_text(size = 11),
    legend.background = element_blank()
  ) +
  labs(
    title = "Democratic Share of Major Party Vote",
    subtitle = "Presidential Election, 2016",
    fill = "2016\nShare",
    caption = "Source: MIT Election Data and Science Lab"
  )
```

## Probabilities

Any good election forecast needs to be _probabilistic_. Professional forecasts take this division
of votes (usually from an aggregate of polls) then calculate the probability distribution around
that range with a series of other factors.

For this simulation, we already have probabilities for `r nrow(market_prices)` states. PredictIt
only hosts markets for the most competitive states. The reality is, the other 
`r 51-nrow(market_prices)` contests are fairly noncompetitive. From the density plot below, we can
see how the 2016 popular vote differed for those states _with_ 2020 markets and those without.

```{r vote_range, echo=FALSE}
last_election %>%
  filter(state != "DC") %>%
  mutate(
    market = if_else(
      condition = state %in% market_prices$state,
      true = "No Market",
      false = "Market Exists"
    )
  ) %>% 
  ggplot(aes(x = share)) + 
  geom_density(aes(fill = market)) +
  facet_wrap(~market, ncol = 1, scales = "free_y") +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    guide = FALSE,
    values = c(
      RColorBrewer::brewer.pal(3, "Dark2")[1],
      RColorBrewer::brewer.pal(3, "Dark2")[2]
    )
  ) +
  labs(
    title = "Popular Vote Difference for Battleground States",
    x = "2016 Popular Vote",
    y = "Number of States"
  ) +
  theme(strip.text = element_text(size = 12))
```

```{r add_sd_rnorm}
last_election <- last_election %>%
  # add state std devs
  left_join(state_sd) %>%
  rowwise() %>%
  # calculate win rate of simulations
  mutate(prob = mean(rnorm(10000, share, sd) > 0.50)) %>% 
  ungroup()
```

If we visualize this process, we can see how the 2016 result and a standard deviation is used
to simulate many elections and calculate a probability.

Below you can see the results of 1,000 simulated elections in Maryland, where
`r scales::percent(last_election$past[last_election$state == "MD"])` of votes were cast for the
Democratic candidate in the last election. The area under the curve past 50% is the _probability_
of a democrat winning again in the next election.

```{r example_range_md, echo=FALSE}
ex_share <- last_election$share[last_election$state == "MD"]
ex_sd <- last_election$sd[last_election$state == "MD"]
n <- 1000
ggplot(data = NULL, mapping = aes(x = rnorm(n = n, mean = ex_share, sd = ex_sd))) + 
  geom_density(fill = RColorBrewer::brewer.pal(3, "Dark2")[3]) +
  geom_vline(xintercept = 0.5, size = 1) +
  geom_vline(xintercept = ex_share, linetype = 2, size = 1) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = sprintf("Maryland 2020 Simulated %s Times", comma(n)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  ) +
  coord_cartesian(xlim = c(0.2, 0.8)) +
  geom_label(aes(x = ex_share, y = 5, label = "2016 Result")) +
  theme(
    axis.text.y.left = element_blank(),
    axis.ticks.y.left = element_blank()
  )
```

Now, lets see the distribution of 1,000 simulated elections in Florida, a much closer election with
only `r scales::percent(last_election$past[last_election$state == "FL"])` of voters supporting the
democratic candidate.

```{r example_range_fl, echo=FALSE}
ex_share <- last_election$share[last_election$state == "FL"]
ex_sd <- last_election$sd[last_election$state == "FL"]
n <- 1000
ggplot(data = NULL, mapping = aes(x = rnorm(n = n, mean = ex_share, sd = ex_sd))) + 
  geom_density(fill = RColorBrewer::brewer.pal(3, "Dark2")[2]) +
  geom_vline(xintercept = 0.5, size = 1) +
  geom_vline(xintercept = ex_share, linetype = 2, size = 1) +
  scale_x_continuous(labels = scales::percent) +
  labs(
    title = sprintf("Folrida 2020 Simulated %s Times", comma(n)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  ) +
  coord_cartesian(xlim = c(0.2, 0.8)) +
  geom_label(aes(x = ex_share, y = 5, label = "2016 Result")) +
  theme(
    axis.text.y.left = element_blank(),
    axis.ticks.y.left = element_blank()
  )
```

We can generate this probability by calculating the average number of simulated elections won by
the democrat. Below, we see how this is done by simulating the Connecticut election 60 times.

```{r example_process}
# find last share
(ex_share <- last_election$share[last_election$state == "CT"])
# find std dev
(ex_sd <- last_election$sd[last_election$state == "CT"])
# simulate 60 elections
(ex_sims <- round(x = rnorm(n = 60, mean = ex_share, sd = ex_sd), digits = 2))
# check for win each each
(ex_wins <- ex_sims > 0.5)
# calculate percent of wins
mean(ex_wins)
```

Below, you can see how the 2016 vote results result in more extreme probabilities.

```{r last_election_hist, echo=FALSE}
last_election %>% 
  mutate(dem = share > 0.5) %>% 
  ggplot(aes(x = share)) +
  geom_histogram(bins = 10, mapping = aes(fill = dem)) +
  geom_vline(xintercept = 0.5, size = 1) +
  labs(
    title = "Distribution of Democratic Shares in 2016",
    subtitle = "Percentage of Majory Party Vote",
    x = "Democratic Share (2016)",
    y = "Number of States"
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = brewer_rdbu[c(1, 5)],
    guide = FALSE
  )
```

```{r sim_prob_hist, echo=FALSE}
last_election %>% 
  mutate(dem = share > 0.5) %>% 
  ggplot(aes(x = prob)) +
  geom_histogram(bins = 10, mapping = aes(fill = dem)) +
  geom_vline(xintercept = 0.5, size = 1) +
  labs(
    title = "Distribution of Simulated Democratic Probability for 2020",
    subtitle = "Mean of 10,000 Random Normally Distributed Simulations",
    x = "Democratic Probability (2020)",
    y = "Number of States"
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = brewer_rdbu[c(1, 5)],
    guide = FALSE
  )
```

This relationship depends entirely on our chosen standard deviation. Again, since we are only
simulating probabilities for those states _without_ markets, these probabilities tend to be extreme.

```{r sim_relationship, echo=FALSE}
last_election %>% 
  filter(state %out% market_prices$state) %>% 
  ggplot(aes(x = share, y = prob)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, fill = prob > 0.5)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = brewer_rdbu[c(1, 5)], guide = FALSE) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  )
```

## Combine Sources

The efficient market hypothesis holds that our markets are a more accurate method to generate
probabilistic predictions. We will uses these market prices over our simulated elections where we
have them.

```{r join_past_price}
state_probs <- last_election %>%
  # add market prices
  left_join(market_prices, by = "state") %>%
  # combine probabilities
  mutate(
    prob = coalesce(price, prob),
    market = !is.na(price)
  ) %>%
  select(state, share, sd, prob, market)
```

```{r sim_relationship_market, echo=FALSE}
state_probs %>% 
  ggplot(aes(x = share, y = prob)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, fill = market)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(3, "Dark2")[c(1, 2)]) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    fill = "Market Price",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  ) +
  theme(legend.position = "bottom")
```

```{r 2020_map, echo=FALSE}
left_join(usa_map, state_probs, by = "state") %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = prob)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "div",
    palette = "RdBu",
    direction = 1,
    label = dollar
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Probability of Democratic Victory",
    subtitle = "Simulated and Market Prices",
    fill = "Probability",
    caption = "Sources: Wikipedia, MIT, & PredictIt"
    )
```

## Electoral College

To simulate the election we of course need to know how many electoral votes are up for grabs in 
each state. We can get this number directly from the National Archive and Records Administration,
the federal agency tasked with overseeing the electoral college.

```{r college_votes}
college_votes <- 
  # read archive.gov website
  read_html("https://www.archives.gov/federal-register/electoral-college/allocation.html") %>% 
  html_nodes("table") %>% 
  # extract table as tibble
  html_table(fill = TRUE) %>% 
  extract2(5) %>% as_tibble() %>%
  # format for join
  set_names(c("state", "votes")) %>% 
  mutate(state = abrev_state(state))
```

We can then add these votes to our data frame of probabilities.

```{r join_votes}
college_probs <- left_join(state_probs, college_votes, by = "state")
arrange(college_probs, desc(votes))
```

To simulate the entire electoral college, we simple have to perform the same `sample()` process as
we did with Connecticut, above. To simplify this process, we can create a new `sim_race()` function
that takes a probability and returns a `TRUE` or `FALSE` indicating whether or not the democrat
has won.

```{r sim_fun}
sim_race <- function(dem = 1-rep, rep = 1-dem) {
  sample(
    size = 1,
    x = c(TRUE, FALSE),
    prob = c(dem, rep)
  )
}
```

If we call this function 100 times on a race with a Democratic probability of 75% then we should
get 75 democratic victories. The more times we run this function, the closer to 75% of the races
the Democrat will win.

```{r example_function}
ex_sim <- rep(NA, 100)
for (i in 1:100) {
  ex_sim[i] <- sim_race(dem = 0.75)
}
ex_sim
mean(ex_sim)
```

With this function, we can also simulate every state in the country at once and count the number of
electoral college votes won by each party.

```{r one_simulation}
ex_sim <- map_lgl(college_probs$prob, sim_race)
sum(college_probs$votes[ex_sim])
```

To best understand the _range_ of possible outcomes, we can perform the same simulation many times.

```{r many_simulations}
n <- 10000
sims <- rep(NA, n)
for (i in seq(1, n)) {
  state_outcomes <- map_lgl(college_probs$prob, sim_race)
  dem_total <- sum(college_probs$votes[state_outcomes])
  sims[i] <- dem_total
}
```

From the summary below, you can see a picture of a very close race with the Democrats holding a 
slight lead. Of our `r comma(n)` simulations, the Democrats won  `r percent(mean(sims > 269))` 
with the modal outcome being a victory of  `r names(sort(table(sims), decreasing = TRUE)[1])` 
electoral college votes.

```{r summary_sims}
# summary of simulations
summary(sims)
# probability of dem victory
mean(sims > 269)
```

```{r sim_hist, echo=FALSE}
ggplot(data = NULL) +
  geom_histogram(mapping = aes(x = sims, fill = sims > 270)) +
  geom_vline(xintercept = 270, size = 1) +
  scale_fill_manual(values = brewer_rdbu[c(1, 5)], guide = FALSE) +
  labs(
    title = sprintf("%s Simulated Electoral College Outcomes", scales::comma(n)),
    x = "Simulated Democratic Votes",
    y = "Number of Simulations"
  ) 
```

## Resources

* https://on.mktw.net/2Zd8QOU
* http://wapo.st/2eUm8cv
* https://en.wikipedia.org/wiki/Redskins_Rule
* https://www.thrillist.com/news/nation/halloween-mask-sales-predict-the-presidential-election
* http://wapo.st/2fnlPr3
* https://fivethirtyeight.com/
* http://53eig.ht/2fIYJK2
* https://en.wikipedia.org/wiki/Futures_contract
* https://www.predictit.org/
* https://github.com/kiernann/models-markets
* https://en.wikipedia.org/wiki/Brier_score
* https://53eig.ht/2IFHxVW
* https://www.r-project.org/
* https://www.tidyverse.org/

[01]: https://on.mktw.net/2Zd8QOU
[02]: http://wapo.st/2eUm8cv
[03]: https://en.wikipedia.org/wiki/Redskins_Rule
[04]: https://www.thrillist.com/news/nation/halloween-mask-sales-predict-the-presidential-election
[05]: http://wapo.st/2fnlPr3
[06]: https://fivethirtyeight.com/
[07]: http://53eig.ht/2fIYJK2
[08]: https://en.wikipedia.org/wiki/Futures_contract
[09]: https://www.predictit.org/
[10]: https://github.com/kiernann/models-markets
[11]: https://en.wikipedia.org/wiki/Brier_score
[12]: https://53eig.ht/2IFHxVW
[07]: https://www.r-project.org/
[08]: https://www.tidyverse.org/
