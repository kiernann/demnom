---
title: "Simulating the 2020 Election With Markets"
output:
  github_document: 
    df_print: tibble
    toc: true
    toc_dept: 2
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, purl=FALSE}
library(knitr)
opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  error = FALSE,
  comment = "#>",
  fig.width = 9,
  fig.height = 5,
  dpi = 300
)
options(width = 99)
set.seed(seed = 05)
```

Election prediction helps party officials, campaign operatives, and journalists interpret campaigns
in a quantitative manner. Understanding trends, uncertainty, and likely outcomes is in invaluable
political tool. For this reason, elections will always be predicted. In the absense of numbers, 
people will latch on to whatever predictive tool they can find. The [stock market][01], 
[key incumbency factors][02], the [Washington Redskins][03], [Halloween mask sales][04], and 
a [psychic Chinese monkeys][05]. If we're going to predict elections, we have a responsibility to
make legitimate quatitative predictions from scientifically founded bases. 

In the past few years, as big data has sought to supplant arbitrary punditry, the forecasting model
has become a staple of political science and journalism. Popularized by the data journalist at
[FiveThirtyEight][06], the forecasting model is a statistical tool used to incorporate a number of
quantitative inputs and produce a _probabilistic_ view of all possible outcomes.

However, following the 2016 Presidential election, the public ([perhaps wrongly][07]) felt betrayed
by the promise of data to predict the future. This left political scientists and journalists alike
reassessing other predictive tools. My favorite of these alternatives is the prediction market.

Prediction markets can be used to generate similarly probabilistic views of election outcomes by
utilizing the economic forces of price discovery and risk aversion to overcome the ideological
bias of self-interested traders on a binary options exchange. Traders use real money to buy shares
of [futures contracts][08] tied to an outcome. The price of these shares fluctuates on the market
as the underlying _likelihood_ of that outcome changes. [PredictIt][09] is an exchange for such contracts, run by Victoria University of Wellington.

Following the 2018 Midterm elections, [I wrote a paper][10] comparing these markets to the
congressional model published by the data journalists at FiveThirtyEight. I found no statistical
difference in the two method's ability to make [skilled predictions][11] over the course of the
Midterm elections. In fact, the markets showed reasonable skepticism in a number of upset
elections. Below, you can see how competitive Congressonal races were predicted by both the markets and model.

![](https://raw.githubusercontent.com/kiernann/models-markets/master/plots/plot_cartesian.png)

With the 2020 Presidential race well under way, the media, voters, campaigns, and political
scientists alike are all looking for the best way to provide useful predictions and avoid the
pitfalls of 2016. This far from the General Election, what little polling we have 
[is less than useless][12]. In the absense of more quantitative data, can we possibly use
prediction markets to generate a useful probabilistic simulation of the electoral college? today 
I'll try and use data from the PredictIt exchange to answer this question.

## Process

I'll be using the open source [language R][07] and packages from the [Tidyverse ecosystem][08].

```{r packages, warning=FALSE}
if (!require("pacman")) install.packages("pacman")
pacman::p_load_current_gh("kiernann/campfin")
pacman::p_load(tidyverse, dataverse, magrittr, jsonlite, scales, rvest)
```

Throughout this simulation, I'll be using only the Democratic party market prices, popular vote,
and probability. It's difficult to determine the winner of an election from a single party's 
share of the popular vote, as only a plurality and not a majority of votes is required. This means
we can't directly test if a party recieves >50% of the vote to determine if they've won, as some
winners recieve less than 50% of the vote. We can get around this by looking at only votes cast for
the major parties (Democrats and Republicans); if a party wins >50% of the major party vote, we can 
pretty safely assume they've won the plurality of votes. This is a reductive, but makes this quick
and dirty simulation a lot easier. A more professional model would fully incorperate third-party
voters, which [can affect elections][09].

## Battleground Data

PredictIt hosts markets for most of the competitive battleground states. We can scrape these
markets using their API and the `jsonlite::fromJSON()` function. 

```{r scrape_markets, warning=FALSE, error=FALSE}
market_prices <-
  # query predictit api
  fromJSON(txt = "https://www.predictit.org/api/marketdata/all/") %>%
  # use markets tree
  use_series(markets) %>%
  # keep only battleground markets
  filter(str_detect(name, "Which party will win (.*) in the 2020 presidential election?")) %>%
  # expose contracts nests has new rows
  unnest(contracts, names_repair = "unique") %>%
  # keep only dem contracts
  filter(shortName...11 == "Democratic") %>%
  # select state and latest price
  select(state = shortName...3, price = lastTradePrice) %>%
  # extract state abbreviation from question
  mutate(state = str_extract(state, "[:upper:]{2}")) %>% 
  arrange(price)
```

From this API, we get probability data for `r nrow(market_prices)` battleground states.

```{r map_markets, echo=FALSE}
brewer_rdbu <- RColorBrewer::brewer.pal(n = 5, name = "RdBu")
usa_map <- map_data("state") %>% mutate(state = abrev_state(region))
left_join(usa_map, market_prices) %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = price)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "div",
    palette = "RdBu",
    direction = 1,
    label = dollar
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank(),
    plot.caption = element_text(size = 11)
  ) +
  labs(
    title = "Battleground Market Prices",
    subtitle = sprintf("Market price of democratic contract on %s", date()),
    caption = "Source: PredictIt",
    fill = "Market\nPrice"
  )
```

We should note the trading volume of each market. Higher volume is typically associated with a more
accurate price equilibrium. That is, the more people trade on the market the more accurate the 
prediction will be. We can get this data by downloading the 90 day history of each market.

```{r market_volume, echo=FALSE}
college_markets <-
  fromJSON(txt = "https://www.predictit.org/api/marketdata/all/") %>%
  use_series(markets) %>%
  filter(str_detect(name, "Which party will win (.*) in the 2020 presidential election?"))

college_states <- str_extract(college_markets$shortName, "[:upper:]{2}")

college_urls <- sprintf(
  "https://www.predictit.org/Resource/DownloadMarketChartData?marketid=%s&timespan=90d",
  college_markets$id
)

map(
  college_urls,
  read_csv,
  col_types = cols(
    ContractName = col_character(),
    Date = col_date("%m/%d/%Y %H:%M:%S %p"),
    OpenSharePrice = col_number(),
    HighSharePrice = col_number(),
    LowSharePrice = col_number(),
    CloseSharePrice = col_number(),
    TradeVolume = col_double()
  )
) %>%
  set_names(value = college_states) %>%
  bind_rows(.id = "state") %>%
  rename(
    name   = ContractName,
    date   = Date,
    open   = OpenSharePrice,
    high   = HighSharePrice,
    low    = LowSharePrice,
    close  = CloseSharePrice,
    volume = TradeVolume
  ) %>% 
  group_by(state) %>% 
  arrange(date) %>% 
  mutate(vol_cum = cumsum(volume)) %>% 
  filter(
    date == max(date),
    name == "Democratic"
  ) %>% 
  right_join(usa_map) %>%
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = vol_cum)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "seq",
    palette = "YlOrBr",
    direction = 1,
    label = comma
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank(),
    plot.caption = element_text(size = 11)
  ) +
  labs(
    title = "Cumulative Democratic Contracts Traded",
    subtitle = "Increased market volume associated with more accurate price equilibrium",
    caption = "Source: PredictIt",
    fill = "Cumulative\nVolume"
  )
```

## Past Elections

These states alone aren't enough to simulate the 2020 electoral college. To predict the
remaining states, we have a few options. The easiest route is to simply assume the party which won
in 2016 will win again in 2020. This isn't neccesarily a terrible idea, as the majority of states
rarely flip, especially not the `r 50-nrow(market_prices)` states without a prediction market. We
can start from this assumption and improve upon it very easily.

To calculate some simple probablistic predictions for the remaining states, we will use data from
the MIT Election Data and Science Lab, which has a database of popular vote results in each state
for every Presidential election since 1976. This file can be read using `dataverse::get_file()`.

```{r read_mit}
past_elections <-
  # get MIT dataverse file
  get_file(
    file = "1976-2016-president.tab", 
    dataset = "doi:10.7910/DVN/42MVDX",
  ) %>%
  # parse raw as data frame
  read_csv(col_types = cols()) %>% 
  rename(votes = candidatevotes) %>%
  # keep only major candidates
  filter(writein == FALSE) %>% 
  filter(party %in% c("democrat", "republican")) %>% 
  # calculate dem share of major vote
  group_by(year, state_po) %>% 
  mutate(share = votes/sum(votes)) %>%
  # keep only dems
  filter(party == "democrat") %>% 
  select(year, state = state_po, share)

# MIT Election Data and Science Lab, 2017, "1976-2016-president.tab"
# https://doi.org/10.7910/DVN/42MVDX/MFU99O, Harvard Dataverse, V5
```

This historical data provides the two statistics needed to finish our probablistic simulation.

First, we have the democratic share of the vote in the last election. This statistic is the default
assumption for our 2020 simulation.

```{r last_election}
last_election <- past_elections %>%
  # keep only last election
  filter(year == 2016)
```

```{r 2016_map, echo=FALSE}
last_election$share[last_election$state %in% c("WY", "WV")] <- 
  last_election$share[last_election$state == "ND"]
left_join(usa_map, last_election, by = "state") %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = share)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "div",
    palette = "RdBu",
    direction = 1,
    label = dollar
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    plot.caption = element_text(size = 11),
    legend.background = element_blank()
  ) +
  labs(
    title = "Democratic Share of Major Party Vote",
    subtitle = "Presidential Election, 2016",
    fill = "2016\nShare",
    caption = "Source: MIT Election Data and Science Lab"
  )

last_election <- filter(past_elections, year == 2016)
```

Second, we can calculate the variation in the party's share of the vote in the last 
`r n_distinct(past_elections$year)` elections. This statistic allows us to introduce uncertainty
to our assumption. The greater the historical variation, the less certain we can be that the winner
of the _last_ election will win the _next_ election.

```{r summarize_sd}
state_sd <- past_elections %>% 
  # calculate state std dev
  group_by(state) %>% 
  summarize(sd = sd(share))
```

```{r map_sd, echo=FALSE}
left_join(usa_map, state_sd, by = "state") %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = sd)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "seq", palette = 5, direction = 1
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Variation in Democratic Share of Major Party Vote",
    subtitle = "Presidential Elections, 1976-2016",
    fill = "Standard\nDeviation",
    caption = "Source: MIT Election Data and Science Lab"
    )
```

## Probabilities

Any good election forecast needs to be _probabilistic_. Professional forecasts take this division
of votes (usually from an aggregate of polls) then calculate the probability distribution around
that division using a series of other factors.

For this simulation, we already have probabilities for `r nrow(market_prices)` states. PredictIt
only hosts markets for the most competitive states. The reality is, the other 
`r 51-nrow(market_prices)` contests are fairly noncompetitive. From the density plot below, we can
see how the 2016 popular vote differed for those states _with_ 2020 markets and those without.

```{r vote_range, echo=FALSE}
last_election %>%
  filter(state != "DC") %>%
  mutate(market = state %in% market_prices$state) %>% 
  ggplot(aes(x = share)) + 
  geom_histogram(aes(fill = market), bins = 9) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(breaks = seq(1, 10, 2)) +
  scale_fill_manual(
    values = c(
      RColorBrewer::brewer.pal(3, "Dark2")[1],
      RColorBrewer::brewer.pal(3, "Dark2")[2]
    )
  ) +
  labs(
    title = "Popular Vote Difference for Battleground States",
    subtitle = "Markets exist for most competitive states",
    caption = "Source: PredictIt",
    x = "2016 Popular Vote",
    y = "Number of States",
    fill = "Market Exists"
  ) +
  theme(
    legend.position = "bottom",
    plot.caption = element_text(size = 11)
  )
```

To turn these Democratic share statistics for less competitive states into probabilities of
Democratic victory, we simply have to add our calculated historical standard deviation and
generate 10,000 random observations from a normal distribution using `rnorm()` with our mean
set to the 2016 share.

```{r add_sd_rnorm}
last_election <- last_election %>%
  # add state std devs
  left_join(state_sd) %>%
  rowwise() %>%
  # calculate win rate of simulations
  mutate(prob = mean(rnorm(10000, share, sd) > 0.50)) %>% 
  ungroup()
```

If we visualize this process, we can see how the 2016 result and a standard deviation is used
to simulate many elections and calculate a probability.

Below you can see the results of 1,000 simulated elections in Vermont, which saw
`r percent(last_election$share[last_election$state == "VT"])` of votes were cast for the 
Democratic candidate in the last election, but has a historical standard deviation of
`r round(last_election$sd[last_election$state == "VT"], 3)`, we can generate 1000 new elections
from our random normal distribution. Any simulated election with a share of the major party vote
greater than 50% is considered a Democratic victory. The percentage of those simulated elections
won is the _probability_ of a Democrat winning the next election.

```{r example_range_md, echo=FALSE}
ex_share <- last_election$share[last_election$state == "VT"]
ex_sd <- last_election$sd[last_election$state == "VT"]
n <- 1000
x <- rnorm(n = n, mean = ex_share, sd = ex_sd)
ggplot(data = NULL, mapping = aes(x = x)) + 
  geom_histogram(aes(fill = x > 0.5), binwidth = 0.02) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 250)) +
  scale_fill_manual(
    values = brewer_rdbu[c(1, 5)],
    guide = FALSE
  ) +
  labs(
    title = sprintf("Vermont 2020 Simulated %s Times", comma(n)),
    subtitle = sprintf("Mean = %s, Standard Deviation = %s", round(ex_share, 3), round(ex_sd, 3)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  )
```

Now, lets see the distribution of 1,000 simulated elections in Pennsylvania, a much closer 
election, where only `r scales::percent(last_election$share[last_election$state == "FL"])` of 
voters supported the democratic candidate but where the historical standard deviation is much 
lower at `r round(last_election$sd[last_election$state == "PA"], 3)`. You can see how the number of
elections below and above 50% is more equal, meaning our simulated probability is much closer to
50%. This is a state where we will use a prediction market.

```{r example_range_fl, echo=FALSE}
ex_share <- last_election$share[last_election$state == "PA"]
ex_sd <- last_election$sd[last_election$state == "PA"]
n <- 1000
x <- rnorm(n = n, mean = ex_share, sd = ex_sd)
ggplot(data = NULL, mapping = aes(x = x)) + 
  geom_histogram(aes(fill = x > 0.5), binwidth = 0.02) +
  scale_x_continuous(labels = scales::percent) +
  coord_cartesian(xlim = c(0, 1), ylim = c(0, 250)) +
  scale_fill_manual(
    values = brewer_rdbu[c(1, 5)],
    guide = FALSE
  ) +
  labs(
    title = sprintf("Pennsylvania 2020 Simulated %s Times", comma(n)),
    subtitle = sprintf("Mean = %s, Standard Deviation = %s", round(ex_share, 3), round(ex_sd, 3)),
    x = "Popular Vote",
    y = "Simulated Occurrences"
  )
```

We can generate this probability by calculating the average number of simulated elections won by
the democrat. Below, we see how this is done by simulating the Connecticut election 60 times.

```{r example_process}
# find std dev
(ex_past <- round(past_elections$share[past_elections$state == "CT"], digits = 3))
(ex_sd <- sd(ex_past))
# find last share
(ex_share <- last_election$share[last_election$state == "CT"])
# simulate 60 elections
(ex_sims <- round(x = rnorm(n = 60, mean = ex_share, sd = ex_sd), digits = 3))
# check for win each each
(ex_wins <- ex_sims > 0.5)
# calculate percent of wins
mean(ex_wins)
```

Below, you can see how the 2016 vote results result in more extreme probabilities.

```{r last_election_hist, echo=FALSE}
last_election %>%
  rename(`Last Election`= share, `Simulated Probability` = prob) %>% 
  gather(`Last Election`, `Simulated Probability`, key = "stat", value = "val") %>% 
  mutate(dem = val > 0.5) %>% 
  ggplot(aes(x = val)) +
  geom_histogram(binwidth = 0.1, mapping = aes(fill = dem)) +
  labs(
    title = "Distribution of Democratic Shares in 2016",
    subtitle = "Percentage of Majory Party Vote",
    x = "Democratic Share (2016)",
    y = "Number of States"
  ) +
  coord_cartesian(xlim = c(0, 1)) +
  scale_x_continuous(labels = scales::percent) +
  scale_fill_manual(
    values = brewer_rdbu[c(1, 5)],
    guide = FALSE
  ) + 
  facet_wrap(~stat, ncol = 1)
```

This relationship depends entirely on our chosen standard deviation. Again, since we are only
simulating probabilities for those states _without_ markets, these probabilities tend to be
extreme.

```{r sim_relationship, echo=FALSE}
last_election %>% 
  filter(state %out% market_prices$state) %>% 
  ggplot(aes(x = share, y = prob)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, fill = prob > 0.5)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = brewer_rdbu[c(1, 5)], guide = FALSE) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  )
```

## Combine Sources

The efficient market hypothesis holds that our markets are a more accurate method to generate
probabilistic predictions. We will uses these market prices over our simulated elections where we
have them.

```{r join_past_price}
state_probs <- last_election %>%
  # add market prices
  left_join(market_prices, by = "state") %>%
  # combine probabilities
  mutate(
    prob = coalesce(price, prob),
    market = !is.na(price)
  ) %>%
  select(state, share, sd, prob, market)
```

```{r sim_relationship_market, echo=FALSE}
state_probs %>% 
  ggplot(aes(x = share, y = prob)) +
  geom_hline(yintercept = 0.5, size = 1) +
  geom_vline(xintercept = 0.5, linetype = 2, size = 1) +
  geom_label(aes(label = state, fill = market)) +
  scale_x_continuous(labels = scales::percent) +
  scale_y_continuous(labels = scales::percent) +
  scale_size_continuous(range = c(2, 10), guide = FALSE) +
  scale_fill_manual(values = RColorBrewer::brewer.pal(3, "Dark2")[c(1, 2)]) +
  labs(
    title = "Popular Vote Relationship to Simulated Probability",
    fill = "Market Price",
    x = "2016 Democratic Vote",
    y = "2020 Democratic Probability"
  ) +
  theme(legend.position = "bottom")
```

We now have probabilities for all 51 electoral contests!

```{r 2020_map, echo=FALSE}
left_join(usa_map, state_probs, by = "state") %>%
  filter(state != "DC") %>% 
  ggplot(mapping = aes(x = long, y = lat, group = group)) +
  geom_polygon(color = "black", mapping = aes(fill = prob)) +
  coord_quickmap() +
  scale_fill_distiller(
    type = "div",
    palette = "RdBu",
    direction = 1,
    label = percent
  ) +
  theme(
    legend.position = c(0.9, 0.35),
    panel.grid = element_blank(),
    axis.text  = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank(),
    legend.background = element_blank()
  ) +
  labs(
    title = "Probability of Democratic Victory",
    subtitle = "Simulated and Market Prices",
    fill = "Probability",
    caption = "Sources: Wikipedia, MIT, & PredictIt"
    )
```

However, this map does not paint the most accurate picture of the election. Presidential elections
are of course conducted with the electoral college, where each state's value is different.

## Electoral College

To simulate the electoral college from these probabilities, we of course still need to know how
many electoral votes are up for grabs in each state. We can get this number directly from the
National Archive and Records Administration, the federal agency tasked with overseeing the
electoral college.

```{r college_votes}
college_votes <- 
  # read archive.gov website
  read_html("https://www.archives.gov/federal-register/electoral-college/allocation.html") %>% 
  html_nodes("table") %>% 
  # extract table as tibble
  html_table(fill = TRUE) %>% 
  extract2(5) %>% as_tibble() %>%
  # format for join
  set_names(c("state", "votes")) %>% 
  mutate(state = abrev_state(state))
```

We can then add these votes to our data frame of probabilities.

```{r join_votes}
college_probs <- left_join(state_probs, college_votes, by = "state")
arrange(college_probs, desc(votes))
```

To simulate the entire electoral college, we simple have to perform the same `sample()` process as
we did with Connecticut above. To simplify this process, we can create a new `sim_race()` function
that takes a probability and returns `TRUE` or `FALSE` indicating whether or not the democrat has
won.

```{r sim_fun}
sim_race <- function(dem = 1-rep, rep = 1-dem) {
  sample(
    size = 1,
    x = c(TRUE, FALSE),
    prob = c(dem, rep)
  )
}
```

If we call this function 100 times on a race with a Democratic probability of 75% then we should
get 75 democratic victories. The more times we run this function, the closer to 75% of the races
the Democrat will win.

```{r example_function}
ex_sim <- rep(NA, 100)
for (i in 1:100) {
  ex_sim[i] <- sim_race(dem = 0.75)
}
print(ex_sim)
mean(ex_sim)
```

With this function, we can also simulate every state in the country at once and count the number of
electoral college votes won by each party.

```{r one_simulation}
ex_sim <- map_lgl(college_probs$prob, sim_race)
sum(college_probs$votes[ex_sim])
```

To best understand the _range_ of possible outcomes, we can perform the same simulation many times.

```{r many_simulations}
n <- 10000
sims <- rep(NA, n)
for (i in seq(1, n)) {
  state_outcomes <- map_lgl(college_probs$prob, sim_race)
  dem_total <- sum(college_probs$votes[state_outcomes])
  sims[i] <- dem_total
}
```

From the summary below, we see a picture of an election where the Democratic party holds a 
significant lead. Of our `r comma(n)` simulations, the Democrats won  `r percent(mean(sims > 269))` 
with the modal outcome being a victory of  `r names(sort(table(sims), decreasing = TRUE)[1])` 
electoral college votes.

```{r summary_sims}
# summary of simulations
summary(sims)
# probability of dem victory
mean(sims > 269)
```

```{r sim_hist, echo=FALSE}
ggplot(data = NULL) +
  geom_histogram(mapping = aes(x = sims, fill = sims > 270)) +
  geom_vline(xintercept = 270, size = 1) +
  scale_fill_manual(values = brewer_rdbu[c(1, 5)], guide = FALSE) +
  labs(
    title = sprintf("%s Simulated Electoral College Outcomes", scales::comma(n)),
    x = "Simulated Democratic Votes",
    y = "Number of Simulations"
  ) 
```

## Resources

* https://on.mktw.net/2Zd8QOU
* http://wapo.st/2eUm8cv
* https://en.wikipedia.org/wiki/Redskins_Rule
* https://www.thrillist.com/news/nation/halloween-mask-sales-predict-the-presidential-election
* http://wapo.st/2fnlPr3
* https://fivethirtyeight.com/
* http://53eig.ht/2fIYJK2
* https://en.wikipedia.org/wiki/Futures_contract
* https://www.predictit.org/
* https://github.com/kiernann/models-markets
* https://en.wikipedia.org/wiki/Brier_score
* https://53eig.ht/2IFHxVW
* https://www.r-project.org/
* https://www.tidyverse.org/

[01]: https://on.mktw.net/2Zd8QOU
[02]: http://wapo.st/2eUm8cv
[03]: https://en.wikipedia.org/wiki/Redskins_Rule
[04]: https://www.thrillist.com/news/nation/halloween-mask-sales-predict-the-presidential-election
[05]: http://wapo.st/2fnlPr3
[06]: https://fivethirtyeight.com/
[07]: http://53eig.ht/2fIYJK2
[08]: https://en.wikipedia.org/wiki/Futures_contract
[09]: https://www.predictit.org/
[10]: https://github.com/kiernann/models-markets
[11]: https://en.wikipedia.org/wiki/Brier_score
[12]: https://53eig.ht/2IFHxVW
[07]: https://www.r-project.org/
[08]: https://www.tidyverse.org/
